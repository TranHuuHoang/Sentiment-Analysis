{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from keras.optimizers import SGD, Adam, Nadam, RMSprop\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D,SimpleRNN,LeakyReLU\n",
    "from keras.layers.core import Dense, Activation,Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard, ReduceLROnPlateau,EarlyStopping\n",
    "from keras.applications import Xception\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainData(path):     #loads data for training.\n",
    "    D = pd.read_csv(path)\n",
    "    feature_names  = np.array(list(D.columns.values))\n",
    "    X_cat = np.array(list(D['Summary'][:50000]))\n",
    "    X_train = np.array(list(D['Text'][:50000]))\n",
    "    for x in range(0,X_cat.shape[0]):\n",
    "         X_train[x] = X_cat[x] + \" \" + X_train[x]      \n",
    "    Y_train = np.array(list(D['Score'][:50000]))\n",
    "    # Make score from 0 to 4\n",
    "    for y in range(0,Y_train.shape[0]):\n",
    "        Y_train[y] = Y_train[y] - 1\n",
    "    return  X_train, Y_train, feature_names\n",
    "\n",
    "def loadTestData(path):     #loads data for testing.\n",
    "    D = pd.read_csv(path)\n",
    "    X_test=np.array(list(D['Text'][55000:60000]))\n",
    "    X_cat = np.array(list(D['Summary'][55000:60000]))\n",
    "    for x in range(0,X_cat.shape[0]):\n",
    "         X_test[x] = X_cat[x] + \" \" + X_test[x]\n",
    "    Y_test = np.array(list(D['Score'][55000:60000]))\n",
    "    # Make score from 0 to 4\n",
    "    for y in range(0,Y_test.shape[0]):\n",
    "        Y_test[y] = Y_test[y] - 1\n",
    "    return  X_test, Y_test\n",
    "\n",
    "\n",
    "def shuffle(a, b): # Shuffles 2 arrays with the same order\n",
    "    s = np.arange(a.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    return a[s], b[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, feature_names = loadTrainData('./Reviews.csv')\n",
    "X_test, Y_test = loadTestData('./Reviews.csv')\n",
    "print('Training data shapes')\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('Y_train.shape: ',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tokenizer = Tokenizer()\n",
    "Tokenizer.fit_on_texts(np.concatenate((X_train, X_test), axis=0))\n",
    "# Tokenizer.fit_on_texts(X_train)\n",
    "Tokenizer_vocab_size = len(Tokenizer.word_index) + 1\n",
    "print(\"Vocab size\",Tokenizer_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#masking\n",
    "num_test = 5000\n",
    "mask = range(num_test)\n",
    "\n",
    "Y_Val = Y_train[:num_test]\n",
    "X_Val = X_train[:num_test]\n",
    "\n",
    "\n",
    "X_train = X_train[num_test:]\n",
    "Y_train = Y_train[num_test:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode text\n",
    "maxWordCount= 1000\n",
    "maxDictionary_size = Tokenizer_vocab_size\n",
    "\n",
    "encoded_Xtrain = Tokenizer.texts_to_sequences(X_train)\n",
    "encoded_Xval = Tokenizer.texts_to_sequences(X_Val)\n",
    "encoded_Xtest = Tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "#padding all text to same size\n",
    "X_Train_encodedPadded = sequence.pad_sequences(encoded_Xtrain, maxlen=maxWordCount)\n",
    "X_Val_encodedPadded = sequence.pad_sequences(encoded_Xval, maxlen=maxWordCount)\n",
    "X_test_encodedPadded = sequence.pad_sequences(encoded_Xtest, maxlen=maxWordCount)\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, 5)\n",
    "Y_Val   = keras.utils.to_categorical(Y_Val, 5)\n",
    "\t\n",
    "\n",
    "shuffle(X_Train_encodedPadded,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature ',feature_names)\n",
    "print(' After extracting a validation set of '+ str(num_test)+'  ')\n",
    "print(' Training data shapes ')\n",
    "print('X_train.shape is ', X_train.shape)\n",
    "print('Y_train.shape is ',Y_train.shape)\n",
    "print(' Validation data shapes ')\n",
    "print('Y_Val.shape is ',Y_Val.shape)\n",
    "print('X_Val.shape is ', X_Val.shape)\n",
    "print(' Test data shape ')\n",
    "print('X_test.shape is ', X_test.shape)\n",
    "\n",
    "print(' After padding all text to same size of '+ str(maxWordCount)+' ')\n",
    "print(' Training data shapes ')\n",
    "print('X_train.shape is ', X_train.shape)\n",
    "print('Y_train.shape is ',Y_train.shape)\n",
    "print(' Validation data shapes ')\n",
    "print('Y_Val.shape is ',Y_Val.shape)\n",
    "print('X_Val.shape is ', X_Val.shape)\n",
    "print(' Test data shape ')\n",
    "print('X_test.shape is ', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(maxDictionary_size, 100, input_length=maxWordCount)) #to change words to ints\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#hidden layers\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128))\n",
    "#model.add(SimpleRNN(30))\n",
    "# model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu',W_constraint=maxnorm(1)))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "# model.add(Dropout(0.6))\n",
    "#model.add(Dense(50, activation='relu',W_constraint=maxnorm(1)))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    " #output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "# adam=Adam(lr=learning_rate, beta_1=0.7, beta_2=0.999, epsilon=1e-08, decay=0.0000001)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "learning_rate=0.0001\n",
    "epochs = 4\n",
    "batch_size = 50\n",
    "#sgd = SGD(lr=learning_rate, nesterov=True, momentum=0.7, decay=1e-4)\n",
    "Nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs/log_25', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./weights/weights_25.hdf5\", verbose=1, save_best_only=True, monitor=\"val_loss\")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=0, verbose=1, mode='auto', cooldown=0, min_lr=1e-6)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Training\"))\n",
    "\n",
    "history  = model.fit(X_Train_encodedPadded, Y_train, epochs = epochs, batch_size=batch_size, verbose=1, validation_data=(X_Val_encodedPadded, Y_Val), callbacks=[tensorboard, reduce_lr, checkpointer, earlyStopping])\n",
    "\n",
    "print((\"Test Score\"))\n",
    "\n",
    "scores = model.evaluate(X_Val_encodedPadded, Y_Val, verbose=0)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print((\"Predicting\"))\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test_encodedPadded, batch_size=batch_size, verbose=1)\n",
    "\n",
    "test_score = accuracy_score(Y_test, predicted_classes)\n",
    "print(\"Test accuracy: %.2f%%\" % (test_score*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
